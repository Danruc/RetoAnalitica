{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"covid19_tweets.csv\")\n",
    "newdf = df.copy()\n",
    "newdf = newdf.drop(['user_name','user_location','user_description','user_created','user_verified','date','text','hashtags','source','is_retweet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_1(s):\n",
    "    s = s.replace('ðŸž', \"\")\n",
    "    s = s.replace('ðŸ¤·â€â™', \"\")\n",
    "    s = s.replace(\"ðŸ”´\", \"\")\n",
    "    s = s.replace(\"€™\", \"\")\n",
    "    s = s.replace(\"ƒ¼\", \"\")\n",
    "    s = s.replace(\"â€˜\", \"\")\n",
    "    s = s.replace(\"â€™\", \"\")\n",
    "    s = s.replace(\"ðŸ˜˜\", \"\")\n",
    "    s = s.replace(\"ðŸ”Š\", \"\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"ðŸ§©\", \"\")\n",
    "    s = s.replace(\":\", \"\")\n",
    "    s = s.replace(\";\", \"\")\n",
    "    s = s.replace(\"'\", \"\")\n",
    "    s = s.replace(\"|\", \"\")\n",
    "    s = s.replace(\"//\", \"\")\n",
    "    s = s.replace('📍',\"\")\n",
    "    s = s.replace('🏫',\"\")\n",
    "    s = s.replace('💯',\"\")\n",
    "    s = s.replace('🧵',\"\")\n",
    "    s = s.replace('🔊',\"\")\n",
    "    s = s.replace(\"●\", \"\")\n",
    "    s = s.replace(\"❤️\", \"\")\n",
    "    s = s.replace(\"🙃\", \"\")\n",
    "    s = s.replace(\"🤔\", \"\")\n",
    "    s = s.replace(\"😉\", \"\")\n",
    "    s = s.replace(\"⬇️\", \"\")\n",
    "    s = s.replace(\"🔴\", \"\")\n",
    "    s = s.replace(\"🤷‍\", \"\")\n",
    "    s = s.replace(\"🐞\", \"\")\n",
    "    s = s.replace(\"🚨\", \"\")\n",
    "    s = s.replace(\"🙌\", \"\")\n",
    "    s = s.replace(\"👑\", \"\")\n",
    "    s = s.replace(\"🔥\", \"\")\n",
    "    s = s.replace(\"👇\", \"\")\n",
    "    s = s.replace(\"👀\", \"\")\n",
    "    s = s.replace(\"/\", \"\")\n",
    "    s = s.replace(\"-\", \"\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.replace(\"https\", \"\")\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_2(s):\n",
    "    s = str(s).upper()\n",
    "    s = s.replace(\"[\", \"\")\n",
    "    s = s.replace(\"]\", \"\")\n",
    "    s = s.replace(\"'\", \"\")\n",
    "    s = s.replace(\"CORONAVIRUS\",\"COVID19\")\n",
    "    s = s.replace(\"CORONAVIRUS, COVID19\",\"COVID19\")\n",
    "    s = s.replace(\"COVID19, COVID19\",\"COVID19\")\n",
    "    \n",
    "    return str(s).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = ''\n",
    "df[\"hashtags_clean\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    df.at[i, \"text_clean\"] = clean_1(row.text)\n",
    "    df.at[i, \"hashtags_clean\"] = clean_2(row.hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "for i, row in df.iterrows():\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if(row[\"text_clean\"] and len(str(row[\"text_clean\"])) < 1000000):\n",
    "        doc = nlp(str(row[\"text_clean\"]))\n",
    "        adjectives = []\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        lemmas = []\n",
    "\n",
    "        for token in doc:\n",
    "            lemmas.append(token.lemma_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.append(token.lemma_)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "                nouns.append(token.lemma_)\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        df.at[i, \"selftext_lemma\"] = \" \".join(lemmas)                \n",
    "        df.at[i, \"selftext_nouns\"] = \" \".join(nouns)\n",
    "        df.at[i, \"selftext_adjectives\"] = \" \".join(adjectives)\n",
    "        df.at[i, \"selftext_verbs\"] = \" \".join(verbs)\n",
    "        df.at[i, \"selftext_nav\"] = \" \".join(nouns+adjectives+verbs)\n",
    "        df.at[i, \"no_tokens\"] = len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by category, count distinct subreddits and posts\n",
    "cat_df = df.groupby('hashtags_clean') \\\n",
    "           .agg({'user_location': pd.Series.nunique,\n",
    "                 'user_name': pd.Series.count}) \\\n",
    "           .rename(columns={'user_location': 'num_locations',\n",
    "                            'user_name': 'num_posts'}) \\\n",
    "           .sort_values('num_posts', ascending=False)\n",
    "            \n",
    "# show top 5 records\n",
    "cat_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df[['num_posts']].plot(kind='box', vert=False, figsize=(6, 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df[['num_locations']].plot(kind='bar', figsize=(7,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame slice\n",
    "sub_df = df[df['hashtags_clean']=='COVID19']\n",
    "\n",
    "# sample cleaned text and tokens tagged as nouns\n",
    "sub_df[['text_clean', 'selftext_nouns']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    return text.split() if text != None else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sub_df.selftext_nouns.map(my_tokenizer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(tokens)\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [t for t in tokens if t not in STOP_WORDS]\n",
    "\n",
    "# rebuild counter\n",
    "counter = Counter(remove_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert list of tuples into data frame\n",
    "freq_df = pd.DataFrame.from_records(counter.most_common(20),\n",
    "                                    columns=['token', 'count'])\n",
    "\n",
    "# create bar plot\n",
    "freq_df.plot(kind='bar', x='token');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wordcloud(counter):\n",
    "    wc = WordCloud(width=1200, height=800, \n",
    "                   background_color=\"white\", \n",
    "                   max_words=200) \n",
    "    wc.generate_from_frequencies(counter)\n",
    "\n",
    "    # Plot\n",
    "    fig=plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_tokens'] = df.selftext_lemma\\\n",
    "  .map(lambda l: 0 if l==None else len(l.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean number of tokens by category\n",
    "df.groupby(['hashtags_clear']) \\\n",
    "  .agg({'no_tokens':'mean'}) \\\n",
    "  .sort_values(by='no_tokens', ascending=False) \\\n",
    "  .plot(kind='bar', figsize=(7,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# render plots as retina or png, because svg is very slow\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def multi_boxplot(data, x, y, ylim = None):\n",
    "    '''Wrapper for sns boxplot with cut-off functionality'''\n",
    "    # plt.figure(figsize=(30, 5))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xticks(rotation=90) \n",
    "\n",
    "    # order boxplots by median\n",
    "    ordered_values = data.groupby(x)[[y]] \\\n",
    "                         .median() \\\n",
    "                         .sort_values(y, ascending=False) \\\n",
    "                         .index\n",
    "        \n",
    "    sns.boxplot(x=x, y=y, data=data, palette='Set2', \n",
    "                order=ordered_values)\n",
    "\n",
    "    fig.set_size_inches(11, 6)\n",
    "    \n",
    "    # cut-off y-axis at value ylim\n",
    "    ax.set_ylim(0, ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boxplot(df, 'category', 'no_tokens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
